<html>
<head>
  <title>Evernote Export</title>
  <basefont face="Tahoma" size="2" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="exporter-version" content="Evernote Windows/303788 (en-US, DDL); Windows/6.1.7601 Service Pack 1 (Win64);"/>
  <style>
    body, td {
      font-family: Tahoma;
      font-size: 10pt;
    }
  </style>
</head>
<body>
<a name="385"/>

<div>
<span><div>所有的层都具有的参数，如name、type、bottom、top和transform_param。</div><div><br/></div><div>本文只讲解视觉层(Vision Layers)的参数，视觉层包括Convolution、Pooling、Local Response Normalization (LRN)、im2col等层。</div><div><br/></div><div><b>1.Convolution层</b></div><div><br/></div><div>就是卷积层，是卷积神经网络(CNN)的核心层。</div><div><br/></div><div>层类型：Convolution</div><div><br/></div><div>     lr_mult: 学习率的系数，最终的学习率是这个数乘以solver.prototxt配置文件中的base_lr。如果有两个lr_mult，则第一个表示权值的学习率，第二个表示偏置项的学习率。一般偏置项的学习率是权值学习率的两倍。</div><div><br/></div><div>     在后面的convolution_param中，我们可以设定卷积层的特有参数。</div><div><br/></div><div>     必须设置的参数：</div><div><br/></div><div>     num_output: 卷积核(filter)的个数</div><div><br/></div><div>     kernel_size: 卷积核的大小。如果卷积核的长和宽不等，需要用kernel_h和kernel_w分别设定</div><div><br/></div><div>     其他参数：</div><div><br/></div><div>     stride: 卷积核的步长，默认为1。也可以用stride_h和stride_w来设置</div><div><br/></div><div>     pad: 扩充边缘，默认为0，不扩充。扩充的时候是左右、上下对称的，比如卷积核的大小为5*5，那么pad设置为2，则四个边缘都扩充2个像素，即宽度和高度都扩充了4个像素，这样卷积运算之后的特征图就不会变小。也可以通过pad_h和pad_w来分别设定。</div><div><br/></div><div>     weight_filter: 权值初始化。默认为&quot;constant&quot;，值全为0，很多时候我们用&quot;xavier&quot;算法来进行初始化，也可以设置为&quot;gaussian&quot;</div><div><br/></div><div>     bias_filter: 偏置项的初始化。一般设置为&quot;constant&quot;，值全为0</div><div><br/></div><div>     bias_term: 是否开启偏置项，默认为true，开启</div><div><br/></div><div>     group: 分组，默认为1组。如果大于1，我们限制卷积的连接操作在一个子集内。如果我们根据图像的通道来分组，那么第i个输出分组只能与第i个输入分组进行连接。</div><div><br/></div><div>     输入：n*c0*w0*h0</div><div><br/></div><div>     输出：n*c1*w1*h1</div><div><br/></div><div>     其中，c1就是参数中的num_output，生成的特征图个数</div><div><br/></div><div>     w1 = (w0+2*pad-kernel_size)/stride+1</div><div><br/></div><div>     h1 = (h0+2*pad-kernel_size)/stride+1</div><div><br/></div><div>     如果设置stride为1，前后两次卷积部分存在折叠。如果设置pad = (kernel_size-1)/2，则运算后，宽度和高度不变</div><div><br/></div><div>     示例：</div><div><br/></div><div>          <span style="color: rgb(168, 168, 168);">layer {</span></div><div><span style="color: rgb(168, 168, 168);">               name: &quot;conv1&quot;</span></div><div><span style="color: rgb(168, 168, 168);">               type: &quot;Convolution&quot;</span></div><div><span style="color: rgb(168, 168, 168);">               bottom: &quot;data&quot;</span></div><div><span style="color: rgb(168, 168, 168);">               top: &quot;conv1&quot;</span></div><div><span style="color: rgb(168, 168, 168);">               param {</span></div><div><span style="color: rgb(168, 168, 168);">                    lr_mult: 1</span></div><div><span style="color: rgb(168, 168, 168);">               }</span></div><div><span style="color: rgb(168, 168, 168);">               param {</span></div><div><span style="color: rgb(168, 168, 168);">                    lr_mult: 2</span></div><div><span style="color: rgb(168, 168, 168);">               }</span></div><div><span style="color: rgb(168, 168, 168);">               convolution_param {</span></div><div><span style="color: rgb(168, 168, 168);">                    num_output: 20</span></div><div><span style="color: rgb(168, 168, 168);">                    kernel_size: 5</span></div><div><span style="color: rgb(168, 168, 168);">                    stride: 1</span></div><div><span style="color: rgb(168, 168, 168);">                    weight_filter {</span></div><div><span style="color: rgb(168, 168, 168);">                         type: &quot;xavier&quot;</span></div><div><span style="color: rgb(168, 168, 168);">                    }</span></div><div><span style="color: rgb(168, 168, 168);">                    bias_filter {</span></div><div><span style="color: rgb(168, 168, 168);">                         type: &quot;constant&quot;</span></div><div><span style="color: rgb(168, 168, 168);">                    }</span></div><div><span style="color: rgb(168, 168, 168);">               }</span></div><div><span style="color: rgb(168, 168, 168);">          }</span></div><div><br/></div><div><b>2.Pooling层</b></div><div><br/></div><div>也叫池化层，为了减少运算量和数据维度而设置的一种层。</div><div><br/></div><div>层类型：Pooling</div><div><br/></div><div>     必须设置的参数：</div><div><br/></div><div>     kernel_size: 池化的核大小。也可以用kernel_h和kernel_w分别设定</div><div><br/></div><div>     其他参数：</div><div><br/></div><div>     pool: 池化方法，默认为MAX。目前可用的方法有MAX、AVE或STOCHASTIC</div><div><br/></div><div>     pad: 和卷积层的pad一样，进行边缘扩充。默认为0</div><div><br/></div><div>     stride: 池化的步长，默认为1。一般我们设置为2，即不重叠。也可以用stride_h和stride_w来设置</div><div><br/></div><div>     示例：</div><div><br/></div><div>          <span style="color: rgb(168, 168, 168);">layer {</span></div><div><span style="color: rgb(168, 168, 168);">               name: &quot;pool1&quot;</span></div><div><span style="color: rgb(168, 168, 168);">               type: &quot;Pooling&quot;</span></div><div><span style="color: rgb(168, 168, 168);">               bottom: &quot;conv1&quot;</span></div><div><span style="color: rgb(168, 168, 168);">               top: &quot;pool1&quot;</span></div><div><span style="color: rgb(168, 168, 168);">               pooling_param {</span></div><div><span style="color: rgb(168, 168, 168);">                    pool: MAX</span></div><div><span style="color: rgb(168, 168, 168);">                    kernel_size: 3</span></div><div><span style="color: rgb(168, 168, 168);">                    stride: 2</span></div><div><span style="color: rgb(168, 168, 168);">               }</span></div><div><span style="color: rgb(168, 168, 168);">          }</span></div><div><br/></div><div>     pooling层的运算方法基本和卷积层是一样的。</div><div><br/></div><div>     输入：n*c*w0*h0</div><div><br/></div><div>     输出：n*c*w1*h1</div><div><br/></div><div>     和卷积层的区别就是其中的c保持不变</div><div><br/></div><div>     w1 = (w0+2*pad-kernel_size)/stride+1</div><div><br/></div><div>     h1 = (h0+2*pad-kernel_size)/stride+1</div><div><br/></div><div>     如果设置stride为2 (同时此处kernel_size为2，pad为0)，前后两次卷积部分不重叠。100*100的特征图池化后，变成50*50。</div><div><br/></div><div><b>3.Local Response Normalization(LRN)层</b></div><div><br/></div><div>此层是对一个输入的局部区域进行归一化，达到&quot;侧抑制&quot;的效果。可以搜索AlexNet或GoogLenet，里面就用到了这个功能。</div><div><br/></div><div>层类型：LRN</div><div><br/></div><div>参数：全部为可选，没有必须</div><div><br/></div><div>     local_size: 默认为5。如果是跨通道LRN，则表示求和的通道数；如果是在通道内LRN，则表示求和的正方形区域长度。</div><div><br/></div><div>     alpha: 默认为1，归一化公式中的参数。</div><div><br/></div><div>     beta：默认为5，归一化公式中的参数。</div><div><br/></div><div>     norm_region: 默认为ACROSS_CHANNELS。有两个选择，ACROSS_CHANNELS表示在相邻的通道间求和归一化。WITHIN_CHANNEL表示在一个通道内部特定的区域内进行求和归一化。与前面的local_size参数对应。</div><div><br/></div><div>归一化公式：</div><div><br/></div><div>     对于每一个输入，去除以</div><div>                                        <img src="2.视觉层(Vision Layers)及参数_files/公式.png" type="image/png" style="height: auto;"/></div><div>     得到归一化后的输出。</div><div><br/></div><div>     示例：</div><div><br/></div><div>          <span style="color: rgb(168, 168, 168);">layer {</span></div><div><span style="color: rgb(168, 168, 168);">               name: &quot;norm1&quot;</span></div><div><span style="color: rgb(168, 168, 168);">               type: &quot;LRN&quot;</span></div><div><span style="color: rgb(168, 168, 168);">               bottom: &quot;pool1&quot;</span></div><div><span style="color: rgb(168, 168, 168);">               top: &quot;norm1&quot;</span></div><div><span style="color: rgb(168, 168, 168);">               lrn_param {</span></div><div><span style="color: rgb(168, 168, 168);">                    local_size: 5</span></div><div><span style="color: rgb(168, 168, 168);">                    alpha: 0.0001</span></div><div><span style="color: rgb(168, 168, 168);">                    beta: 0.75</span></div><div><span style="color: rgb(168, 168, 168);">               }</span></div><div><span style="color: rgb(168, 168, 168);">          }</span></div><div><br/></div><div><b>4.im2col层</b></div><div><br/></div><div>如果对matlab比较熟悉的话，就应该知道im2col是什么意思。它先将一个大矩阵，重叠地划分为多个子矩阵，对每个子矩阵序列化成向量，最后得到另外一个矩阵。</div><div><br/></div><div>看一看图就知道了：</div><div><br/></div><div>     <img src="2.视觉层(Vision Layers)及参数_files/图1.png" type="image/png" style="height: auto;"/></div><div><br/></div><div>在caffe中，卷积运算就是先对数据进行im2col操作，在进行内积运算(inner product)。这样做，比原始的卷积操作速度更快。</div><div><br/></div><div>看看两种卷积操作的异同：</div><div><br/></div><div>     <img src="2.视觉层(Vision Layers)及参数_files/图2.png" type="image/png" style="height: auto;"/></div><div><br/></div><div><br/></div></span>
</div></body></html> 