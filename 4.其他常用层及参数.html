<html>
<head>
  <title>Evernote Export</title>
  <basefont face="Tahoma" size="2" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="exporter-version" content="Evernote Windows/303788 (en-US, DDL); Windows/6.1.7601 Service Pack 1 (Win64);"/>
  <style>
    body, td {
      font-family: Tahoma;
      font-size: 10pt;
    }
  </style>
</head>
<body>
<a name="399"/>

<div>
<span><div>本文讲解一些其他的常用层，包括：Softmax_loss层，Inner Product层，Accuracy层，Reshape层和Dropout层及其它们的参数配置。</div><div><br/></div><div><b>1.softmax-loss</b></div><div><br/></div><div>softmax-loss层和softmax层计算大致是相同的。softmax是一个分类器，计算的是类别的概率(Likelihood)，是Logistic Regression的一种推广。Logistic Regression只能用于二分类，而softmax可以用于多分类。</div><div><br/></div><div>softmax与softmax-loss的区别：</div><div><br/></div><div>     softmax计算公式：</div><div><br/></div><div>          <img src="4.其他常用层及参数_files/softmax.png" type="image/png" style="height: auto;"/></div><div><br/></div><div>     而softmax-loss计算公式：</div><div><br/></div><div>          <img src="4.其他常用层及参数_files/softmax-loss.png" type="image/png" style="height: auto;"/></div><div><br/></div><div>关于两者的区别更加具体的介绍，可参考：softmax vs. softmax-loss(<a href="http://freemind.pluskid.org/machine-learning/softmax-vs-softmax-loss-numerical-stability/">http://freemind.pluskid.org/machine-learning/softmax-vs-softmax-loss-numerical-stability/</a>)</div><div><br/></div><div>用户可能最终目的就是得到各个类别的概率似然值，这个时候就只需要一个softmax层，而不一定需要进行softmax-loss操作；或者是用户有通过其他什么方式已经得到了某种概率似然值，然后要做最大似然估计，此时则只需要后面的softmax-loss而不需要前面的softmax操作。因此提供两个不同的layer结构比只提供一个合在一起的softmax-loss layer要灵活许多。</div><div><br/></div><div>不管是softmax layer还是softmax-loss layer，都是没有参数的，只是层类型不同而已。</div><div><br/></div><div>     softmax-loss layer: 输出loss值</div><div><br/></div><div>          <span style="color: rgb(168, 168, 168);">layer {</span></div><div><span style="color: rgb(168, 168, 168);">               name: &quot;loss&quot;</span></div><div><span style="color: rgb(168, 168, 168);">               type: &quot;SoftmaxWithLoss&quot;</span></div><div><span style="color: rgb(168, 168, 168);">               bottom: &quot;ip1&quot;</span></div><div><span style="color: rgb(168, 168, 168);">               bottom: &quot;label&quot;</span></div><div><span style="color: rgb(168, 168, 168);">               top: &quot;loss&quot;</span></div><div><span style="color: rgb(168, 168, 168);">          }</span></div><div><br/></div><div>     softmax layer: 输出似然值</div><div><br/></div><div>          <span style="color: rgb(168, 168, 168);">layer {</span></div><div><span style="color: rgb(168, 168, 168);">               name: &quot;prob&quot;</span></div><div><span style="color: rgb(168, 168, 168);">               type: &quot;Softmax&quot;</span></div><div><span style="color: rgb(168, 168, 168);">               bottom: &quot;cls3_fc&quot;</span></div><div><span style="color: rgb(168, 168, 168);">               top: &quot;prob&quot;</span></div><div><span style="color: rgb(168, 168, 168);">          }</span></div><div><br/></div><div><b>2.Inner Product</b></div><div><br/></div><div>全连接层，把输入当作一个向量，输出也是一个简单向量(把输入数据的blob的width和height全变为1)。</div><div><br/></div><div>输入：n*c0*h*w</div><div><br/></div><div>输出：n*c1*1*1</div><div><br/></div><div>全连接层实际上也是一种卷积层，只是它的卷积核大小和原始数据大小一致。因此它的参数基本和卷积层的参数一样。</div><div><br/></div><div>层类型：InnerProduct</div><div><br/></div><div>lr_mult: 学习率的系数，最终的学习率是这个数乘以solver.prototxt配置文件中的base_lr。如果有两个lr_mult，则第一个表示权值的学习率，第二个表示偏置项的学习率。一般偏置项的学习率是权值学习率的两倍。</div><div><br/></div><div>必须设置的参数：</div><div><br/></div><div>     num_output: 过滤器(filter)的个数</div><div><br/></div><div>其他参数：</div><div><br/></div><div>     weight_filter: 权值初始化。默认为&quot;constant&quot;，值全为0，很多时候我们用&quot;xavier&quot;算法来进行初始化，也可以设置&quot;gaussian&quot;</div><div><br/></div><div>     bias_filter: 偏置项的初始化。一般设置为&quot;constant&quot;，值全为0</div><div><br/></div><div>     bias_term: 是否开启偏置项，默认为true，开启</div><div><br/></div><div>示例：</div><div><br/></div><div>     <span style="color: rgb(168, 168, 168);">layer {</span></div><div><span style="color: rgb(168, 168, 168);">          name: &quot;ip1&quot;</span></div><div><span style="color: rgb(168, 168, 168);">          type: &quot;InnerProduct&quot;</span></div><div><span style="color: rgb(168, 168, 168);">          bottom: &quot;pool2&quot;</span></div><div><span style="color: rgb(168, 168, 168);">          top: &quot;ip1&quot;</span></div><div><span style="color: rgb(168, 168, 168);">          param {</span></div><div><span style="color: rgb(168, 168, 168);">               lr_mult: 1</span></div><div><span style="color: rgb(168, 168, 168);">          }</span></div><div><span style="color: rgb(168, 168, 168);">          param {</span></div><div><span style="color: rgb(168, 168, 168);">               lr_mult: 2</span></div><div><span style="color: rgb(168, 168, 168);">          }</span></div><div><span style="color: rgb(168, 168, 168);">          inner_product_param {</span></div><div><span style="color: rgb(168, 168, 168);">               num_output: 500</span></div><div><span style="color: rgb(168, 168, 168);">               weight_filter {</span></div><div><span style="color: rgb(168, 168, 168);">                    type: &quot;xavier&quot;</span></div><div><span style="color: rgb(168, 168, 168);">               }</span></div><div><span style="color: rgb(168, 168, 168);">               bias_filter {</span></div><div><span style="color: rgb(168, 168, 168);">                    type: &quot;constant&quot;</span></div><div><span style="color: rgb(168, 168, 168);">               }</span></div><div><span style="color: rgb(168, 168, 168);">          }</span></div><div><span style="color: rgb(168, 168, 168);">     }</span></div><div><br/></div><div><b>3.accuracy</b></div><div><br/></div><div>输出分类(预测)精确度，只有test阶段才有，因此需要加入include参数。</div><div><br/></div><div>层类型：Accuracy</div><div><br/></div><div>示例：</div><div><br/></div><div>     <span style="color: rgb(168, 168, 168);">layer {</span></div><div><span style="color: rgb(168, 168, 168);">          name: &quot;accuracy&quot;</span></div><div><span style="color: rgb(168, 168, 168);">          type: &quot;Accuracy&quot;</span></div><div><span style="color: rgb(168, 168, 168);">          bottom: &quot;ip2&quot;</span></div><div><span style="color: rgb(168, 168, 168);">          bottom: &quot;label&quot;</span></div><div><span style="color: rgb(168, 168, 168);">          top: &quot;accuracy&quot;</span></div><div><span style="color: rgb(168, 168, 168);">          include {</span></div><div><span style="color: rgb(168, 168, 168);">               phase: TEST</span></div><div><span style="color: rgb(168, 168, 168);">          }</span></div><div><span style="color: rgb(168, 168, 168);">     }</span></div><div><br/></div><div><b>4.reshape</b></div><div><br/></div><div>在不改变数据的情况下，改变输入的维度。</div><div><br/></div><div>层类型：Reshape</div><div><br/></div><div>先来看例子：</div><div><br/></div><div>     <span style="color: rgb(168, 168, 168);">layer {</span></div><div><span style="color: rgb(168, 168, 168);">          name: &quot;reshape&quot;</span></div><div><span style="color: rgb(168, 168, 168);">          type: &quot;Reshape&quot;</span></div><div><span style="color: rgb(168, 168, 168);">          bottom: &quot;input&quot;</span></div><div><span style="color: rgb(168, 168, 168);">          top: &quot;output&quot;</span></div><div><span style="color: rgb(168, 168, 168);">          reshape_param {</span></div><div><span style="color: rgb(168, 168, 168);">               shape {</span></div><div><span style="color: rgb(168, 168, 168);">                    dim: 0 <span style="color: rgb(77, 206, 29);"># copy the dimension from below</span></span></div><div><span style="color: rgb(168, 168, 168);">                    dim: 2</span></div><div><span style="color: rgb(168, 168, 168);">                    dim: 3</span></div><div><span style="color: rgb(168, 168, 168);">                    dim: -1 <span style="color: rgb(77, 206, 29);"># infer it from the other dimensions</span></span></div><div><span style="color: rgb(168, 168, 168);">               }</span></div><div><span style="color: rgb(168, 168, 168);">          }</span></div><div><span style="color: rgb(168, 168, 168);">     }</span></div><div><br/></div><div>有一个可选的参数组shape，用于指定blob数据的各维的值(blob是一个四维的数据：n*c*w*h)。</div><div><br/></div><div>     dim: 0     表示维度不变，即输入和输出是相同的维度。</div><div><br/></div><div>     dim: 2 和 dim: 3     将原来的维度变成2或3。</div><div><br/></div><div>     dim: -1     表示由系统自动计算维度。数据的总量不变，系统会根据blob数据的其他三维来自动计算当前维的维度值。</div><div><br/></div><div>假设原数据为：64*3*28*28，表示64张3通道的28*28的彩色图片。经过reshape变换：</div><div><br/></div><div>     <span style="color: rgb(168, 168, 168);">reshape_param {</span></div><div><span style="color: rgb(168, 168, 168);">          shape {</span></div><div><span style="color: rgb(168, 168, 168);">               dim: 0</span></div><div><span style="color: rgb(168, 168, 168);">               dim: 0</span></div><div><span style="color: rgb(168, 168, 168);">               dim: 14</span></div><div><span style="color: rgb(168, 168, 168);">               dim: -1</span></div><div><span style="color: rgb(168, 168, 168);">          }</span></div><div><span style="color: rgb(168, 168, 168);">     }</span></div><div><br/></div><div>输出数据为64*3*14*56。</div><div><br/></div><div><b>5.Dropout</b></div><div><br/></div><div>Dropout是一个防止过拟合的trick。可以随机让网络某些隐含层节点的权重不工作。</div><div><br/></div><div>先看例子：</div><div><br/></div><div>     <span style="color: rgb(168, 168, 168);">layer {</span></div><div><span style="color: rgb(168, 168, 168);">          name: &quot;drop7&quot;</span></div><div><span style="color: rgb(168, 168, 168);">          type: &quot;Dropout&quot;</span></div><div><span style="color: rgb(168, 168, 168);">          bottom: &quot;fc7-conv&quot;</span></div><div><span style="color: rgb(168, 168, 168);">          top: &quot;fc7-conv&quot;</span></div><div><span style="color: rgb(168, 168, 168);">          dropout_param {</span></div><div><span style="color: rgb(168, 168, 168);">               dropout_ratio: 0.5</span></div><div><span style="color: rgb(168, 168, 168);">          }</span></div><div><span style="color: rgb(168, 168, 168);">     }</span></div><div><br/></div><div>只需要设置一个dropout_ratio就可以了。</div><div><br/></div><div>还有其他更多的层，但用的地方 不多，就不一一介绍了。</div><div><br/></div><div>随着深度学习的深入，各种各样的新模型会不断出现，因此对应的各种新类型的层也在不断地出现。这些新出现的层，我们只有在等caffe更新到新版本后，再去慢慢地摸索了。</div></span>
</div></body></html> 