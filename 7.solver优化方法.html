<html>
<head>
  <title>Evernote Export</title>
  <basefont face="微软雅黑" size="2" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="exporter-version" content="Evernote Windows/303788 (zh-CN, DDL); Windows/6.1.7601 Service Pack 1 (Win64);"/>
  <style>
    body, td {
      font-family: 微软雅黑;
      font-size: 10pt;
    }
  </style>
</head>
<body>
<a name="407"/>

<div>
<span><div>上文提到，到目前为止，caffe总共提供了六种优化方法：</div><div><br/></div><ul><li>Stochastic Gradient Descent (type: &quot;SGD&quot;)</li><li>AdaDelta (type: &quot;AdaDelta&quot;)</li><li>Adaptive Gradient (type: &quot;AdaGrad&quot;)</li><li>Adam (type: &quot;Adam&quot;)</li><li>Nesterov's Accelerated Gradient (type: &quot;Nesterov&quot;)</li><li>RMSProp (type: &quot;RMSProp&quot;)</li></ul><div><br/></div><div>Solver就是用来使loss最小化的优化方法。对于一个数据集D，需要优化的目标函数是整个数据集中所有数据loss的平均值。</div><div><br/></div><div>                         <img src="7.solver优化方法_files/loss_function.png" type="image/png" style="height: auto;"/></div><div><br/></div><div>其中， f<sub>W</sub>(x<sup>(i)</sup>)计算的是数据 x<sup>(i)</sup>上的loss，先将每个单独的样本x的loss求出来，然后求和，最后求均值。r(W)是正则项(weight_decay)，为了减弱过拟合现象。</div><div><br/></div><div>如果采用这种Loss函数，迭代一次需要计算整个数据集，在数据集非常大的情况下，这种方法的效率很低，这个也是我们熟知的梯度下降采用的方法。</div><div><br/></div><div>在实际中，通过将整个数据集分成几批(batches)，每一批就是一个mini-batch，其数量(batch_size)为N&lt;&lt;|D|，此时的loss函数为：</div><div><br/></div><div>                         <img src="7.solver优化方法_files/batch_loss_function.png" type="image/png" style="height: auto;"/></div><div><br/></div><div>有了Loss函数后，就可迭代的求解loss和梯度来优化这个问题。在神经网络中，用forward pass来求解loss，用backward pass来求解梯度。</div><div><br/></div><div>在caffe中，默认采用的Stochastic Gradient Descent (SGD)进行优化求解。后面几种方法也是基于梯度的优化方法(like SGD)，因此本文只介绍一下SGD，其他的方法，有兴趣的同学可以去看相关文献。</div><div><br/></div><div>1.Stochastic Gradient Descent (SGD)</div><div><br/></div><div>随机梯度下降(Stochastic Gradient Descent)是在梯度下降法(Gradient Descent)的基础上发展起来的，梯度下降法也叫最速下降法。SGD在通过负梯度<img src="7.solver优化方法_files/负梯度.png" type="image/png" style="height: auto;"/>和上一次的权重更新值 V<sub>t</sub>的线性组合来更新W，迭代公式如下：</div><div><br/></div><div>                         <img src="7.solver优化方法_files/W更新.png" type="image/png" style="height: auto;"/></div><div><br/></div><div>其中，<img src="7.solver优化方法_files/alpha.png" type="image/png" style="height: auto;"/>是负梯度的学习率(base_lr)，<img src="7.solver优化方法_files/miu.png" type="image/png" style="height: auto;"/>是上一次梯度值的权重(momentum)，用来加权之前梯度方向对现在梯度下降方向的影响。这两个参数需要通过tuning来得到最好的结果，一般是根据经验设定的。如果你不知道如何设定这些参数，可以参看相关的论文。</div><div><br/></div><div>在深度学习中使用SGD，比较好的初始化参数的策略是把学习率设为0.01左右(base_lr：0.01)，在训练的过程中，如果loss开始出现稳定水平时，对学习率乘以一个常数因子(gamma)，这样的过程重复多次。</div><div><br/></div><div>对于momentum，一般取值在0.5~0.99之间。通常设为0.9，momentum可以让使用SGD的深度学习方法更加稳定以及快速。</div><div><br/></div><div>关于更多的momentum，请参看Hinton的《A Practical Guide to Training Restricted Boltzmann Machines》。</div><div><br/></div><div>示例：</div><div><br/></div><div style="-en-codeblock: true; box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-top-left-radius: 4px; border-top-right-radius: 4px; border-bottom-right-radius: 4px; border-bottom-left-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.14902); background-position: initial initial; background-repeat: initial initial;"><div>base_lr: 0.01</div><div>lr_policy: &quot;step&quot;</div><div>gamma: 0.1</div><div>stepsize: 1000</div><div>max_iter: 3500</div><div>momentum: 0.9</div></div><div><br/></div><div>lr_policy设置为step，则学习率的变化规则为base_lr * gamma ^ (floor(iter / stepsize))。</div><div><br/></div><div>即前1000次迭代，学习率为0.01；第1001~2000次迭代，学习率为0.001；第2001~3000次迭代，学习率为0.0001；第3001~3500次迭代，学习率为 10<sup>-5</sup>。</div><div><br/></div><div>上面的设置只能作为一种指导，它们不能保证在任何情况下都能得到最佳的结果，有时候这种方法甚至不work。如果学习的时候出现diverge(比如，你一开始就发现非常大或者NaN或者INF的loss值或者输出)，此时你需要降低base_lr的值(比如，0.001)，然后重新训练，这样的过程重复几次直到你找到可以work的base_lr。</div><div><br/></div><div>2.AdaDelta</div><div><br/></div><div>AdaDelta是一种鲁棒的学习率方法，是基于梯度的优化方法(like SGD)。</div><div><br/></div><div>示例：</div><div><br/></div><div style="-en-codeblock: true; box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-top-left-radius: 4px; border-top-right-radius: 4px; border-bottom-right-radius: 4px; border-bottom-left-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.14902); background-position: initial initial; background-repeat: initial initial;"><div>net: &quot;examples/mnist/lenet_train_test.prototxt&quot;</div><div>test_iter: 100</div><div>test_interval: 500</div><div>base_lr: 1.0</div><div>lr_policy: &quot;fixed&quot;</div><div>momentum: 0.95</div><div>weight_decay: 0.0005</div><div>display: 100</div><div>max_iter: 1000</div><div>snapshot: 5000</div><div>snapshot_prefix: &quot;examples/mnist/lenet_adadelta&quot;</div><div>solver_mode: GPU</div><div>type: &quot;AdaDelta&quot;</div><div>delta: 1e-6</div></div><div><br/></div><div>从最后两行可看出，设置solver type为AdaDelta时，需要设置delta的值。</div><div><br/></div><div>3.AdaGrad</div><div><br/></div><div>自适应梯度(Adaptive Gradient)是基于梯度的优化方法(like SGD)。</div><div><br/></div><div>示例：</div><div><br/></div><div style="-en-codeblock: true; box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-top-left-radius: 4px; border-top-right-radius: 4px; border-bottom-right-radius: 4px; border-bottom-left-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.14902); background-position: initial initial; background-repeat: initial initial;"><div>net: &quot;examples/mnist/mnist_autoencoder.prototxt&quot;</div><div>test_state: { stage: 'test-on-train' }</div><div>test_iter: 500</div><div>test_state: { stage: 'test-on-test' }</div><div>test_iter: 100</div><div>test_interval: 500</div><div>test_compute_loss: true</div><div>base_lr: 0.01</div><div>lr_policy: &quot;fixed&quot;</div><div>display: 100</div><div>max_iter: 65000</div><div>weight_decay: 0.0005</div><div>snapshot: 10000</div><div>snapshot_prefix: &quot;examples/mnist/mnist_autocoder_adagrad_train&quot;</div><div># solver_mode: GPU or CPU</div><div>solver_mode: GPU</div><div>type: &quot;AdaGrad&quot;</div></div><div><br/></div><div>4.Adam</div><div><br/></div><div>是一种基于梯度的优化方法(like SGD)。</div><div><br/></div><div>5.NAG</div><div><br/></div><div>Nesterov的加速梯度法(Nesterov's Accelerated Gradient)作为凸优化中最理想的方法，其收敛速度非常快。</div><div><br/></div><div>示例：</div><div><br/></div><div style="-en-codeblock: true; box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-top-left-radius: 4px; border-top-right-radius: 4px; border-bottom-right-radius: 4px; border-bottom-left-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.14902); background-position: initial initial; background-repeat: initial initial;"><div>net: &quot;examples/mnist/mnist_autoencoder.prototxt&quot;</div><div>test_state: { stage: 'test-on-train' }</div><div>test_iter: 500</div><div>test_state: { stage: 'test-on-test' }</div><div>test_iter: 100</div><div>test_interval: 500</div><div>test_compute_loss: true</div><div>base_lr: 0.01</div><div>lr_policy: &quot;step&quot;</div><div>gamma: 0.1</div><div>stepsize: 10000</div><div>display: 100</div><div>max_iter: 65000</div><div>weight_decay: 0.0005</div><div>snapshot: 10000</div><div>snapshot_prefix: &quot;examples/mnist/mnist_autoencoder_nesterov_train&quot;</div><div>momentum: 0.95</div><div># solver mode: CPU or GPU</div><div>solver_mode: GPU</div><div>type: &quot;Nesterov&quot;</div></div><div><br/></div><div>6.RMSProp</div><div><br/></div><div>RMSProp是Tieleman在一次Coursera课程演讲中提出来的，也是一种基于梯度的优化方法(like SGD)。</div><div><br/></div><div>示例：</div><div><br/></div><div style="-en-codeblock: true; box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-top-left-radius: 4px; border-top-right-radius: 4px; border-bottom-right-radius: 4px; border-bottom-left-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.14902); background-position: initial initial; background-repeat: initial initial;"><div>net: &quot;examples/mnist/lenet_train_test.prototxt&quot;</div><div>test_iter: 100</div><div>test_interval: 500</div><div>base_lr: 1.0</div><div>lr_policy: &quot;fixed&quot;</div><div>momentum: 0.95</div><div>weight_decay: 0.0005</div><div>display: 100</div><div>max_iter: 10000</div><div>snapshot: 5000</div><div>snapshot_prefix: &quot;examples/mnist/lenet_adadelta&quot;</div><div>solver_mode: GPU</div><div>type: &quot;RMSProp&quot;</div><div>rms_decay: 0.98</div></div><div><br/></div><div>最后两行，需要设置rms_decay值。</div><div><br/></div></span>
</div></body></html> 